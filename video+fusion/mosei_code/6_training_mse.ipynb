{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cae6880",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/huixian/.conda/envs/multiood/lib/python3.12/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0650 | Val Loss: 1.3405\n",
      "Macro-F1: 0.2183 | Micro-F1: 0.4868 | Acc: 0.4868 | Recall: [0. 1. 0.]\n",
      "✅ Best model saved at epoch 0 with Macro-F1=0.2183\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0478 | Val Loss: 1.3480\n",
      "Macro-F1: 0.2183 | Micro-F1: 0.4868 | Acc: 0.4868 | Recall: [0. 1. 0.]\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0271 | Val Loss: 1.3827\n",
      "Macro-F1: 0.2151 | Micro-F1: 0.4721 | Acc: 0.4721 | Recall: [0.         0.96987952 0.        ]\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0124 | Val Loss: 1.3389\n",
      "Macro-F1: 0.2887 | Micro-F1: 0.4868 | Acc: 0.4868 | Recall: [0.         0.90963855 0.15      ]\n",
      "✅ Best model saved at epoch 3 with Macro-F1=0.2887\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 11/66 [00:04<00:17,  3.12it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gpu_ids = [1]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, gpu_ids))\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- SETTINGS ----\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "clip_dir = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI-Seg/Clip/Clips_16frames\"\n",
    "mapping_csv = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI-Seg/Clip/clip_balanced_split.csv\"\n",
    "\n",
    "batch_size = 16\n",
    "clip_len = 16\n",
    "num_epochs = 20\n",
    "\n",
    "# ---- DATASET ----\n",
    "class VideoClipDataset(Dataset):\n",
    "    def __init__(self, clip_dir, csv_path, feature_extractor):\n",
    "        self.clip_dir = clip_dir\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.samples = list(self.df.itertuples(index=False))  # FIXED\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.samples[idx]\n",
    "        clip_path = os.path.join(self.clip_dir, row.clip_filename)\n",
    "\n",
    "        cap = cv2.VideoCapture(clip_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame[:, :, ::-1])  # BGR to RGB\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < clip_len:\n",
    "            frames += [frames[-1]] * (clip_len - len(frames))\n",
    "        frames = frames[:clip_len]\n",
    "\n",
    "        inputs = self.feature_extractor(images=frames, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        return inputs, torch.tensor(row.sentiment_score, dtype=torch.float32)\n",
    "\n",
    "# ---- LOSS ----\n",
    "class CenteredWeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        ideal = torch.zeros_like(targets)\n",
    "        ideal[targets < -0.3] = -3.0\n",
    "        ideal[targets > 0.3] = 3.0\n",
    "        ideal[(-0.3 <= targets) & (targets <= 0.3)] = 0.0\n",
    "\n",
    "        weights = torch.ones_like(targets)\n",
    "        weights[targets < -0.3] = 2.0\n",
    "        weights[targets > 0.3] = 2.0\n",
    "        weights[(-0.3 <= targets) & (targets <= 0.3)] = 1.0\n",
    "\n",
    "        mse = (preds - ideal) ** 2\n",
    "        return (weights * mse).mean()\n",
    "\n",
    "# ---- MODEL ----\n",
    "class SentimentRegressor(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regressor(x).squeeze(1)\n",
    "\n",
    "# ---- TRAINING UTILS ----\n",
    "def run_epoch(model, loader, optimizer, is_train=True):\n",
    "    model.train() if is_train else model.eval()\n",
    "    total_preds, total_labels = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    for clips, targets in tqdm(loader, leave=False):\n",
    "        clips, targets = clips.to(device), targets.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            features = video_mae(clips).last_hidden_state.mean(dim=1)\n",
    "            preds = model(features)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_preds.extend(preds.detach().cpu().numpy())\n",
    "        total_labels.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    return total_loss / len(loader), np.array(total_preds), np.array(total_labels)\n",
    "\n",
    "def evaluate(preds, labels):\n",
    "    def to_label(x):\n",
    "        return \"Negative\" if x < -0.3 else \"Positive\" if x > 0.3 else \"Neutral\"\n",
    "    preds_label = [to_label(p) for p in preds]\n",
    "    labels_label = [to_label(l) for l in labels]\n",
    "\n",
    "    macro_f1 = f1_score(labels_label, preds_label, average=\"macro\")\n",
    "    micro_f1 = f1_score(labels_label, preds_label, average=\"micro\")\n",
    "    recall = recall_score(labels_label, preds_label, average=None, labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    acc = accuracy_score(labels_label, preds_label)\n",
    "    return macro_f1, micro_f1, recall, acc\n",
    "\n",
    "# ---- FEATURE EXTRACTOR ----\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "video_mae = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "video_mae.eval()\n",
    "for param in video_mae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ---- LOAD DATASET AND SPLIT BASED ON CSV ----\n",
    "full_dataset = VideoClipDataset(clip_dir, mapping_csv, feature_extractor)\n",
    "\n",
    "# Load the CSV again to fetch split info per clip\n",
    "df = pd.read_csv(mapping_csv)\n",
    "\n",
    "# Extract clip-level split indices\n",
    "train_indices = df[df['split'] == 'train'].index.tolist()\n",
    "val_indices   = df[df['split'] == 'val'].index.tolist()\n",
    "test_indices  = df[df['split'] == 'test'].index.tolist()\n",
    "\n",
    "# Create datasets using clip-level splits\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset   = Subset(full_dataset, val_indices)\n",
    "test_dataset  = Subset(full_dataset, test_indices)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---- MODEL SETUP ----\n",
    "regressor = SentimentRegressor(feature_dim=768).to(device)\n",
    "loss_fn = nn.MSELoss()  # or use CenteredWeightedMSELoss()\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=2e-4)\n",
    "\n",
    "# ---- TRAIN LOOP ----\n",
    "best_macro_f1 = -np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    train_loss, _, _ = run_epoch(regressor, train_loader, optimizer, is_train=True)\n",
    "    val_loss, val_preds, val_labels = run_epoch(regressor, val_loader, optimizer, is_train=False)\n",
    "\n",
    "    macro_f1, micro_f1, recall, acc = evaluate(val_preds, val_labels)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(regressor.state_dict(), \"best_regressor_MSE_final.pth\")\n",
    "        print(f\"✅ Best model saved at epoch {epoch} with Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "# ---- TEST EVALUATION ----\n",
    "test_loss, test_preds, test_labels = run_epoch(regressor, test_loader, optimizer, is_train=False)\n",
    "macro_f1, micro_f1, recall, acc = evaluate(test_preds, test_labels)\n",
    "print(\"\\n----- TEST RESULTS -----\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
