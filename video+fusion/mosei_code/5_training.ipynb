{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8005f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gpu_ids = [4]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, gpu_ids))\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_video\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96427338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "clip_dir = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/Clips_16frames\"\n",
    "mapping_csv = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/clip_sentiment_mapping.csv\"\n",
    "save_model_path = \"./best_model_2.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60e51556",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Sampling Customization ---\n",
    "num_positive = 1700\n",
    "num_neutral = 1700\n",
    "num_negative = 1700\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bebb189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Sample Dataset ---\n",
    "df = pd.read_csv(mapping_csv)\n",
    "\n",
    "def classify_sentiment(score):\n",
    "    if score < -0.3:\n",
    "        return \"Negative\"\n",
    "    elif score > 0.3:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "df[\"sentiment_label\"] = df[\"sentiment_score\"].apply(classify_sentiment)\n",
    "\n",
    "samples = []\n",
    "for label, n_sample in zip([\"Positive\", \"Neutral\", \"Negative\"], [num_positive, num_neutral, num_negative]):\n",
    "    subset = df[df[\"sentiment_label\"] == label]\n",
    "    n_sample = min(n_sample, len(subset))\n",
    "    samples.append(subset.sample(n=n_sample, random_state=42))\n",
    "\n",
    "df_sampled = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "total_len = len(df_sampled)\n",
    "train_len = int(total_len * train_ratio)\n",
    "val_len = int(total_len * val_ratio)\n",
    "test_len = total_len - train_len - val_len\n",
    "\n",
    "df_shuffled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df_train = df_shuffled.iloc[:train_len]\n",
    "df_val = df_shuffled.iloc[train_len:train_len+val_len]\n",
    "df_test = df_shuffled.iloc[train_len+val_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01275b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataset ---\n",
    "class VideoClipDataset(Dataset):\n",
    "    def __init__(self, dataframe, clip_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.clip_dir = clip_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        clip_path = os.path.join(self.clip_dir, row[\"clip_filename\"])\n",
    "        video, _, _ = read_video(clip_path, pts_unit=\"sec\")\n",
    "        video = video.permute(0, 3, 1, 2)  # (T, C, H, W)\n",
    "        video = video.float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "\n",
    "        sentiment_score = torch.tensor(row[\"sentiment_score\"], dtype=torch.float32)\n",
    "\n",
    "        return video, sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53f7078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Transform ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "])\n",
    "\n",
    "train_dataset = VideoClipDataset(df_train, clip_dir, transform=transform)\n",
    "val_dataset = VideoClipDataset(df_val, clip_dir, transform=transform)\n",
    "test_dataset = VideoClipDataset(df_test, clip_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c5a4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/huixian/.conda/envs/multiood/lib/python3.12/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- Model ---\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "videomae = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "videomae.eval()\n",
    "\n",
    "class SentimentRegressor(nn.Module):\n",
    "    def __init__(self, feature_dim=768):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "regressor = SentimentRegressor().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25254c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optimizer and Loss ---\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "def evaluate(preds, labels):\n",
    "    preds_label = []\n",
    "    labels_label = []\n",
    "\n",
    "    for p in preds:\n",
    "        if p < -0.3:\n",
    "            preds_label.append(\"Negative\")\n",
    "        elif p > 0.3:\n",
    "            preds_label.append(\"Positive\")\n",
    "        else:\n",
    "            preds_label.append(\"Neutral\")\n",
    "\n",
    "    for l in labels:\n",
    "        if l < -0.3:\n",
    "            labels_label.append(\"Negative\")\n",
    "        elif l > 0.3:\n",
    "            labels_label.append(\"Positive\")\n",
    "        else:\n",
    "            labels_label.append(\"Neutral\")\n",
    "\n",
    "    macro_f1 = f1_score(labels_label, preds_label, average=\"macro\")\n",
    "    micro_f1 = f1_score(labels_label, preds_label, average=\"micro\")\n",
    "    recall = recall_score(labels_label, preds_label, average=None, labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    acc = accuracy_score(labels_label, preds_label)\n",
    "\n",
    "    return macro_f1, micro_f1, recall, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dc2b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 447/447 [01:45<00:00,  4.25it/s]\n",
      "Validating Epoch 0: 100%|██████████| 96/96 [00:22<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Val Macro-F1=0.1974, Micro-F1=0.3333, Acc=0.3333, Recall=[0.02692308 0.94509804 0.028     ]\n",
      "✅ Best model saved at epoch 0 with Macro-F1=0.1974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 447/447 [01:33<00:00,  4.80it/s]\n",
      "Validating Epoch 1: 100%|██████████| 96/96 [00:21<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Val Macro-F1=0.2093, Micro-F1=0.3373, Acc=0.3373, Recall=[0.00769231 0.93333333 0.072     ]\n",
      "✅ Best model saved at epoch 1 with Macro-F1=0.2093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 447/447 [01:44<00:00,  4.29it/s]\n",
      "Validating Epoch 2: 100%|██████████| 96/96 [00:22<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Val Macro-F1=0.2599, Micro-F1=0.3608, Acc=0.3608, Recall=[0.04615385 0.91372549 0.124     ]\n",
      "✅ Best model saved at epoch 2 with Macro-F1=0.2599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 447/447 [01:55<00:00,  3.88it/s]\n",
      "Validating Epoch 3: 100%|██████████| 96/96 [00:23<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Val Macro-F1=0.2742, Micro-F1=0.3608, Acc=0.3608, Recall=[0.05384615 0.87058824 0.16      ]\n",
      "✅ Best model saved at epoch 3 with Macro-F1=0.2742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 447/447 [02:09<00:00,  3.45it/s]\n",
      "Validating Epoch 4: 100%|██████████| 96/96 [00:29<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Val Macro-F1=0.2865, Micro-F1=0.3621, Acc=0.3621, Recall=[0.05384615 0.82745098 0.208     ]\n",
      "✅ Best model saved at epoch 4 with Macro-F1=0.2865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 447/447 [02:09<00:00,  3.44it/s]\n",
      "Validating Epoch 5: 100%|██████████| 96/96 [00:28<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Val Macro-F1=0.2879, Micro-F1=0.3634, Acc=0.3634, Recall=[0.06538462 0.83921569 0.188     ]\n",
      "✅ Best model saved at epoch 5 with Macro-F1=0.2879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 447/447 [02:11<00:00,  3.41it/s]\n",
      "Validating Epoch 6: 100%|██████████| 96/96 [00:29<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Val Macro-F1=0.3017, Micro-F1=0.3791, Acc=0.3791, Recall=[0.13461538 0.89019608 0.112     ]\n",
      "✅ Best model saved at epoch 6 with Macro-F1=0.3017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 447/447 [02:00<00:00,  3.71it/s]\n",
      "Validating Epoch 7: 100%|██████████| 96/96 [00:21<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Val Macro-F1=0.3014, Micro-F1=0.3725, Acc=0.3725, Recall=[0.08461538 0.84705882 0.188     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 447/447 [01:29<00:00,  5.01it/s]\n",
      "Validating Epoch 8: 100%|██████████| 96/96 [00:19<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Val Macro-F1=0.3107, Micro-F1=0.3712, Acc=0.3712, Recall=[0.06923077 0.77254902 0.276     ]\n",
      "✅ Best model saved at epoch 8 with Macro-F1=0.3107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 447/447 [02:05<00:00,  3.57it/s]\n",
      "Validating Epoch 9: 100%|██████████| 96/96 [00:30<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Val Macro-F1=0.3188, Micro-F1=0.3804, Acc=0.3804, Recall=[0.10769231 0.83137255 0.204     ]\n",
      "✅ Best model saved at epoch 9 with Macro-F1=0.3188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 447/447 [02:54<00:00,  2.57it/s]\n",
      "Validating Epoch 10: 100%|██████████| 96/96 [00:38<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Val Macro-F1=0.3322, Micro-F1=0.3882, Acc=0.3882, Recall=[0.11923077 0.82352941 0.224     ]\n",
      "✅ Best model saved at epoch 10 with Macro-F1=0.3322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 447/447 [02:53<00:00,  2.58it/s]\n",
      "Validating Epoch 11: 100%|██████████| 96/96 [00:38<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Val Macro-F1=0.3328, Micro-F1=0.3922, Acc=0.3922, Recall=[0.16923077 0.85490196 0.152     ]\n",
      "✅ Best model saved at epoch 11 with Macro-F1=0.3328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 447/447 [02:52<00:00,  2.59it/s]\n",
      "Validating Epoch 12: 100%|██████████| 96/96 [00:38<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Val Macro-F1=0.3432, Micro-F1=0.3935, Acc=0.3935, Recall=[0.13076923 0.80784314 0.244     ]\n",
      "✅ Best model saved at epoch 12 with Macro-F1=0.3432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 447/447 [02:52<00:00,  2.59it/s]\n",
      "Validating Epoch 13: 100%|██████████| 96/96 [00:38<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Val Macro-F1=0.3410, Micro-F1=0.3987, Acc=0.3987, Recall=[0.18846154 0.85882353 0.148     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 447/447 [02:52<00:00,  2.59it/s]\n",
      "Validating Epoch 14: 100%|██████████| 96/96 [00:38<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Val Macro-F1=0.3527, Micro-F1=0.4039, Acc=0.4039, Recall=[0.17307692 0.84313725 0.196     ]\n",
      "✅ Best model saved at epoch 14 with Macro-F1=0.3527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 447/447 [02:51<00:00,  2.60it/s]\n",
      "Validating Epoch 15: 100%|██████████| 96/96 [00:38<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Val Macro-F1=0.3504, Micro-F1=0.3987, Acc=0.3987, Recall=[0.16153846 0.81960784 0.216     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 447/447 [02:10<00:00,  3.42it/s]\n",
      "Validating Epoch 16: 100%|██████████| 96/96 [00:29<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Val Macro-F1=0.3605, Micro-F1=0.4065, Acc=0.4065, Recall=[0.19230769 0.82745098 0.2       ]\n",
      "✅ Best model saved at epoch 16 with Macro-F1=0.3605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 447/447 [02:23<00:00,  3.12it/s]\n",
      "Validating Epoch 17: 100%|██████████| 96/96 [00:38<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Val Macro-F1=0.3549, Micro-F1=0.4013, Acc=0.4013, Recall=[0.18846154 0.81960784 0.196     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 447/447 [02:51<00:00,  2.61it/s]\n",
      "Validating Epoch 18: 100%|██████████| 96/96 [00:38<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Val Macro-F1=0.3537, Micro-F1=0.3974, Acc=0.3974, Recall=[0.17692308 0.8        0.216     ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 447/447 [02:51<00:00,  2.61it/s]\n",
      "Validating Epoch 19: 100%|██████████| 96/96 [00:37<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Val Macro-F1=0.3593, Micro-F1=0.3948, Acc=0.3948, Recall=[0.15384615 0.7372549  0.296     ]\n",
      "✅ Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "best_macro_f1 = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "    regressor.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for videos, scores in tqdm(train_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        videos = videos.to(device, non_blocking=True)\n",
    "        scores = scores.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = videomae(videos).last_hidden_state[:, 0]  # CLS token\n",
    "\n",
    "        preds = regressor(features).squeeze()\n",
    "\n",
    "        loss = criterion(preds, scores)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    regressor.eval()\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for videos, scores in tqdm(val_loader, desc=f\"Validating Epoch {epoch}\"):\n",
    "            videos = videos.to(device, non_blocking=True)\n",
    "            scores = scores.to(device, non_blocking=True)\n",
    "\n",
    "            features = videomae(videos).last_hidden_state[:, 0]\n",
    "            preds = regressor(features).squeeze()\n",
    "\n",
    "            preds_list.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(scores.cpu().numpy())\n",
    "\n",
    "    macro_f1, micro_f1, recall, acc = evaluate(preds_list, labels_list)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Val Macro-F1={macro_f1:.4f}, Micro-F1={micro_f1:.4f}, Acc={acc:.4f}, Recall={recall}\")\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(regressor.state_dict(), save_model_path)\n",
    "        print(f\"✅ Best model saved at epoch {epoch} with Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "print(\"✅ Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
