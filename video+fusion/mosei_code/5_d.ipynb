{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f84a423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/huixian/.conda/envs/multiood/lib/python3.12/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10.5385 | Val Loss: 10.5506\n",
      "Macro-F1: 0.3732 | Micro-F1: 0.4400 | Acc: 0.4400 | Recall: [0.11564626 0.80208333 0.30434783]\n",
      "✅ Best model saved at epoch 0 with Macro-F1=0.3732\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 10.1457 | Val Loss: 10.1286\n",
      "Macro-F1: 0.4506 | Micro-F1: 0.4560 | Acc: 0.4560 | Recall: [0.46938776 0.546875   0.33540373]\n",
      "✅ Best model saved at epoch 1 with Macro-F1=0.4506\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9.8161 | Val Loss: 10.0076\n",
      "Macro-F1: 0.3969 | Micro-F1: 0.4240 | Acc: 0.4240 | Recall: [0.24489796 0.25       0.79503106]\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9.5110 | Val Loss: 9.5394\n",
      "Macro-F1: 0.4376 | Micro-F1: 0.4360 | Acc: 0.4360 | Recall: [0.51020408 0.30729167 0.52173913]\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9.3182 | Val Loss: 9.3753\n",
      "Macro-F1: 0.4405 | Micro-F1: 0.4420 | Acc: 0.4420 | Recall: [0.49659864 0.28645833 0.57763975]\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9.1759 | Val Loss: 9.3849\n",
      "Macro-F1: 0.4638 | Micro-F1: 0.4640 | Acc: 0.4640 | Recall: [0.58503401 0.33854167 0.50310559]\n",
      "✅ Best model saved at epoch 5 with Macro-F1=0.4638\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 9.0053 | Val Loss: 9.2183\n",
      "Macro-F1: 0.4055 | Micro-F1: 0.4220 | Acc: 0.4220 | Recall: [0.42176871 0.17708333 0.71428571]\n",
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.8487 | Val Loss: 9.0413\n",
      "Macro-F1: 0.4413 | Micro-F1: 0.4480 | Acc: 0.4480 | Recall: [0.51020408 0.25       0.62732919]\n",
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.7282 | Val Loss: 9.0182\n",
      "Macro-F1: 0.4838 | Micro-F1: 0.4860 | Acc: 0.4860 | Recall: [0.57142857 0.328125   0.59627329]\n",
      "✅ Best model saved at epoch 8 with Macro-F1=0.4838\n",
      "\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.5595 | Val Loss: 9.7730\n",
      "Macro-F1: 0.4448 | Micro-F1: 0.4500 | Acc: 0.4500 | Recall: [0.7414966  0.28125    0.38509317]\n",
      "\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.5015 | Val Loss: 8.8003\n",
      "Macro-F1: 0.4459 | Micro-F1: 0.4560 | Acc: 0.4560 | Recall: [0.53741497 0.22916667 0.65217391]\n",
      "\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.3798 | Val Loss: 8.8044\n",
      "Macro-F1: 0.4696 | Micro-F1: 0.4760 | Acc: 0.4760 | Recall: [0.56462585 0.27604167 0.63354037]\n",
      "\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.2130 | Val Loss: 8.9078\n",
      "Macro-F1: 0.4610 | Micro-F1: 0.4640 | Acc: 0.4640 | Recall: [0.63945578 0.27083333 0.53416149]\n",
      "\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.1520 | Val Loss: 8.5906\n",
      "Macro-F1: 0.4632 | Micro-F1: 0.4740 | Acc: 0.4740 | Recall: [0.55782313 0.24479167 0.67080745]\n",
      "\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.0617 | Val Loss: 8.6513\n",
      "Macro-F1: 0.4392 | Micro-F1: 0.4600 | Acc: 0.4600 | Recall: [0.44897959 0.19791667 0.7826087 ]\n",
      "\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.9157 | Val Loss: 8.5190\n",
      "Macro-F1: 0.4504 | Micro-F1: 0.4640 | Acc: 0.4640 | Recall: [0.49659864 0.22395833 0.72049689]\n",
      "\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.8848 | Val Loss: 8.6215\n",
      "Macro-F1: 0.4384 | Micro-F1: 0.4600 | Acc: 0.4600 | Recall: [0.42857143 0.19791667 0.80124224]\n",
      "\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.7665 | Val Loss: 8.3983\n",
      "Macro-F1: 0.4728 | Micro-F1: 0.4860 | Acc: 0.4860 | Recall: [0.5170068  0.25       0.73913043]\n",
      "\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.6442 | Val Loss: 8.3860\n",
      "Macro-F1: 0.4675 | Micro-F1: 0.4780 | Acc: 0.4780 | Recall: [0.58503401 0.23958333 0.66459627]\n",
      "\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5461 | Val Loss: 8.3209\n",
      "Macro-F1: 0.4649 | Micro-F1: 0.4760 | Acc: 0.4760 | Recall: [0.59183673 0.234375   0.65838509]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- TEST RESULTS -----\n",
      "Macro-F1: 0.4629 | Micro-F1: 0.4740 | Acc: 0.4740 | Recall: [0.58       0.22751323 0.66459627]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gpu_ids = [4]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, gpu_ids))\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- SETTINGS ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "clip_dir = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/Clips_16frames\"\n",
    "mapping_csv = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/clip_sentiment_mapping.csv\"\n",
    "\n",
    "negative_samples = 1500\n",
    "neutral_samples = 2000\n",
    "positive_samples = 1500\n",
    "batch_size = 16\n",
    "clip_len = 16\n",
    "num_epochs = 20\n",
    "\n",
    "# ---- DATASET ----\n",
    "class VideoClipDataset(Dataset):\n",
    "    def __init__(self, clip_dir, csv_path, feature_extractor):\n",
    "        self.clip_dir = clip_dir\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        # Group by sentiment label\n",
    "        grouped = self.df.groupby(\"sentiment_label\")\n",
    "\n",
    "        self.samples = []\n",
    "        for label, n_samples in zip([\"Negative\", \"Neutral\", \"Positive\"], [negative_samples, neutral_samples, positive_samples]):\n",
    "            group = grouped.get_group(label)\n",
    "            if label == \"Negative\":\n",
    "                sorted_group = group.sort_values(\"sentiment_score\")\n",
    "            elif label == \"Neutral\":\n",
    "                sorted_group = group.reindex((group[\"sentiment_score\"] - 0).abs().sort_values().index)\n",
    "            else:  # Positive\n",
    "                sorted_group = group.sort_values(\"sentiment_score\", ascending=False)\n",
    "\n",
    "            selected = sorted_group.head(n_samples)\n",
    "            self.samples.extend(selected.itertuples(index=False))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.samples[idx]\n",
    "        clip_path = os.path.join(self.clip_dir, row.clip_filename)\n",
    "\n",
    "        cap = cv2.VideoCapture(clip_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame[:, :, ::-1])  # BGR to RGB\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < clip_len:\n",
    "            frames += [frames[-1]] * (clip_len - len(frames))\n",
    "        frames = frames[:clip_len]\n",
    "\n",
    "        inputs = self.feature_extractor(images=frames, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        return inputs, torch.tensor(row.sentiment_score, dtype=torch.float32)\n",
    "\n",
    "# ---- LOSS ----\n",
    "class CenteredWeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        ideal = torch.zeros_like(targets)\n",
    "        ideal[targets < -0.3] = -3.0\n",
    "        ideal[targets > 0.3] = 3.0\n",
    "        ideal[(-0.3 <= targets) & (targets <= 0.3)] = 0.0\n",
    "\n",
    "        weights = torch.ones_like(targets)\n",
    "        weights[targets < -0.3] = 2.0\n",
    "        weights[targets > 0.3] = 2.0\n",
    "        weights[(-0.3 <= targets) & (targets <= 0.3)] = 1.0\n",
    "\n",
    "        mse = (preds - ideal) ** 2\n",
    "        return (weights * mse).mean()\n",
    "\n",
    "# ---- MODEL ----\n",
    "class SentimentRegressor(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regressor(x).squeeze(1)\n",
    "\n",
    "# ---- TRAINING LOOP ----\n",
    "def run_epoch(model, loader, optimizer, is_train=True):\n",
    "    model.train() if is_train else model.eval()\n",
    "    total_preds, total_labels = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    for clips, targets in tqdm(loader, leave=False):\n",
    "        clips, targets = clips.to(device), targets.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            features = video_mae(clips).last_hidden_state.mean(dim=1)\n",
    "            preds = model(features)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_preds.extend(preds.detach().cpu().numpy())\n",
    "        total_labels.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    return total_loss / len(loader), np.array(total_preds), np.array(total_labels)\n",
    "\n",
    "def evaluate(preds, labels):\n",
    "    def to_label(x):\n",
    "        return \"Negative\" if x < -0.3 else \"Positive\" if x > 0.3 else \"Neutral\"\n",
    "    preds_label = [to_label(p) for p in preds]\n",
    "    labels_label = [to_label(l) for l in labels]\n",
    "\n",
    "    macro_f1 = f1_score(labels_label, preds_label, average=\"macro\")\n",
    "    micro_f1 = f1_score(labels_label, preds_label, average=\"micro\")\n",
    "    recall = recall_score(labels_label, preds_label, average=None, labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    acc = accuracy_score(labels_label, preds_label)\n",
    "    return macro_f1, micro_f1, recall, acc\n",
    "\n",
    "# ---- MAIN ----\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "video_mae = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "video_mae.eval()\n",
    "for param in video_mae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "full_dataset = VideoClipDataset(clip_dir, mapping_csv, feature_extractor)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.1 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "regressor = SentimentRegressor(feature_dim=768).to(device)\n",
    "loss_fn = CenteredWeightedMSELoss()\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=2e-4)\n",
    "\n",
    "# ---- TRAIN ----\n",
    "best_macro_f1 = -np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    train_loss, _, _ = run_epoch(regressor, train_loader, optimizer, is_train=True)\n",
    "    val_loss, val_preds, val_labels = run_epoch(regressor, val_loader, optimizer, is_train=False)\n",
    "\n",
    "    macro_f1, micro_f1, recall, acc = evaluate(val_preds, val_labels)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(regressor.state_dict(), \"best_regressor_neutral.pth\")\n",
    "        print(f\"✅ Best model saved at epoch {epoch} with Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "# ---- EVALUATE TEST ----\n",
    "test_loss, test_preds, test_labels = run_epoch(regressor, test_loader, optimizer, is_train=False)\n",
    "macro_f1, micro_f1, recall, acc = evaluate(test_preds, test_labels)\n",
    "print(\"\\n----- TEST RESULTS -----\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
