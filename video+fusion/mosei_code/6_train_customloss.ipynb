{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2921594f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/huixian/.conda/envs/multiood/lib/python3.12/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.2192 | Val Loss: 8.0449\n",
      "Macro-F1: 0.1570 | Micro-F1: 0.3080 | Acc: 0.3080 | Recall: [0. 1. 0.]\n",
      "✅ Best model saved at epoch 0 with Macro-F1=0.1570\n",
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.1263 | Val Loss: 7.9876\n",
      "Macro-F1: 0.2618 | Micro-F1: 0.3415 | Acc: 0.3415 | Recall: [0.17985612 0.86610879 0.03088803]\n",
      "✅ Best model saved at epoch 1 with Macro-F1=0.2618\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 8.0016 | Val Loss: 8.0122\n",
      "Macro-F1: 0.2536 | Micro-F1: 0.3015 | Acc: 0.3015 | Recall: [0.04316547 0.65271967 0.25482625]\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.9029 | Val Loss: 7.9657\n",
      "Macro-F1: 0.3136 | Micro-F1: 0.3608 | Acc: 0.3608 | Recall: [0.48561151 0.55230126 0.05019305]\n",
      "✅ Best model saved at epoch 3 with Macro-F1=0.3136\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.7921 | Val Loss: 7.9426\n",
      "Macro-F1: 0.3468 | Micro-F1: 0.3518 | Acc: 0.3518 | Recall: [0.26978417 0.53556485 0.27027027]\n",
      "✅ Best model saved at epoch 4 with Macro-F1=0.3468\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.6486 | Val Loss: 8.0491\n",
      "Macro-F1: 0.3291 | Micro-F1: 0.3750 | Acc: 0.3750 | Recall: [0.64748201 0.36820084 0.08880309]\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.5810 | Val Loss: 8.1350\n",
      "Macro-F1: 0.3379 | Micro-F1: 0.3505 | Acc: 0.3505 | Recall: [0.17625899 0.38493724 0.50579151]\n",
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.4459 | Val Loss: 8.0161\n",
      "Macro-F1: 0.3607 | Micro-F1: 0.3686 | Acc: 0.3686 | Recall: [0.48561151 0.35564854 0.25482625]\n",
      "✅ Best model saved at epoch 7 with Macro-F1=0.3607\n",
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.3174 | Val Loss: 8.0823\n",
      "Macro-F1: 0.3622 | Micro-F1: 0.3621 | Acc: 0.3621 | Recall: [0.38129496 0.34728033 0.35521236]\n",
      "✅ Best model saved at epoch 8 with Macro-F1=0.3622\n",
      "\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.2261 | Val Loss: 8.1720\n",
      "Macro-F1: 0.3567 | Micro-F1: 0.3570 | Acc: 0.3570 | Recall: [0.32733813 0.33891213 0.40540541]\n",
      "\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.1289 | Val Loss: 8.1605\n",
      "Macro-F1: 0.3572 | Micro-F1: 0.3621 | Acc: 0.3621 | Recall: [0.45323741 0.29288703 0.32818533]\n",
      "\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.0683 | Val Loss: 8.3930\n",
      "Macro-F1: 0.3271 | Micro-F1: 0.3634 | Acc: 0.3634 | Recall: [0.65827338 0.22175732 0.17760618]\n",
      "\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.9861 | Val Loss: 8.3131\n",
      "Macro-F1: 0.3662 | Micro-F1: 0.3686 | Acc: 0.3686 | Recall: [0.35611511 0.30962343 0.43629344]\n",
      "✅ Best model saved at epoch 12 with Macro-F1=0.3662\n",
      "\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8725 | Val Loss: 8.7071\n",
      "Macro-F1: 0.3393 | Micro-F1: 0.3647 | Acc: 0.3647 | Recall: [0.20503597 0.23849372 0.65250965]\n",
      "\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8216 | Val Loss: 8.4818\n",
      "Macro-F1: 0.3588 | Micro-F1: 0.3686 | Acc: 0.3686 | Recall: [0.32374101 0.24686192 0.52895753]\n",
      "\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.7497 | Val Loss: 8.3761\n",
      "Macro-F1: 0.3642 | Micro-F1: 0.3686 | Acc: 0.3686 | Recall: [0.4028777  0.27196653 0.42084942]\n",
      "\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.6786 | Val Loss: 8.4171\n",
      "Macro-F1: 0.3698 | Micro-F1: 0.3737 | Acc: 0.3737 | Recall: [0.38848921 0.28451883 0.44015444]\n",
      "✅ Best model saved at epoch 16 with Macro-F1=0.3698\n",
      "\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5705 | Val Loss: 8.3892\n",
      "Macro-F1: 0.3498 | Micro-F1: 0.3608 | Acc: 0.3608 | Recall: [0.5        0.23430962 0.32818533]\n",
      "\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5754 | Val Loss: 8.5695\n",
      "Macro-F1: 0.3677 | Micro-F1: 0.3763 | Acc: 0.3763 | Recall: [0.33093525 0.26359833 0.52895753]\n",
      "\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.4604 | Val Loss: 8.5989\n",
      "Macro-F1: 0.3651 | Micro-F1: 0.3750 | Acc: 0.3750 | Recall: [0.3381295  0.24686192 0.53281853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- TEST RESULTS -----\n",
      "Macro-F1: 0.3856 | Micro-F1: 0.4154 | Acc: 0.4154 | Recall: [0.36       0.17410714 0.66192171]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gpu_ids = [4]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, gpu_ids))\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- SETTINGS ----\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "clip_dir = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/Clips_16frames\"\n",
    "mapping_csv = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/clip_sentiment_split.csv\"\n",
    "\n",
    "batch_size = 16\n",
    "clip_len = 16\n",
    "num_epochs = 20\n",
    "\n",
    "# ---- DATASET ----\n",
    "class VideoClipDataset(Dataset):\n",
    "    def __init__(self, clip_dir, csv_path, feature_extractor):\n",
    "        self.clip_dir = clip_dir\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.samples = list(self.df.itertuples(index=False))  # FIXED\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.samples[idx]\n",
    "        clip_path = os.path.join(self.clip_dir, row.clip_filename)\n",
    "\n",
    "        cap = cv2.VideoCapture(clip_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame[:, :, ::-1])  # BGR to RGB\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < clip_len:\n",
    "            frames += [frames[-1]] * (clip_len - len(frames))\n",
    "        frames = frames[:clip_len]\n",
    "\n",
    "        inputs = self.feature_extractor(images=frames, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        return inputs, torch.tensor(row.sentiment_score, dtype=torch.float32)\n",
    "\n",
    "# ---- LOSS ----\n",
    "class CenteredWeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        ideal = torch.zeros_like(targets)\n",
    "        ideal[targets < -0.3] = -3.0\n",
    "        ideal[targets > 0.3] = 3.0\n",
    "        ideal[(-0.3 <= targets) & (targets <= 0.3)] = 0.0\n",
    "\n",
    "        weights = torch.ones_like(targets)\n",
    "        weights[targets < -0.3] = 1.3\n",
    "        weights[targets > 0.3] = 1.3\n",
    "        weights[(-0.3 <= targets) & (targets <= 0.3)] = 1.0\n",
    "\n",
    "        mse = (preds - ideal) ** 2\n",
    "        return (weights * mse).mean()\n",
    "\n",
    "# ---- MODEL ----\n",
    "class SentimentRegressor(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regressor(x).squeeze(1)\n",
    "\n",
    "# ---- TRAINING UTILS ----\n",
    "def run_epoch(model, loader, optimizer, is_train=True):\n",
    "    model.train() if is_train else model.eval()\n",
    "    total_preds, total_labels = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    for clips, targets in tqdm(loader, leave=False):\n",
    "        clips, targets = clips.to(device), targets.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            features = video_mae(clips).last_hidden_state.mean(dim=1)\n",
    "            preds = model(features)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_preds.extend(preds.detach().cpu().numpy())\n",
    "        total_labels.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    return total_loss / len(loader), np.array(total_preds), np.array(total_labels)\n",
    "\n",
    "def evaluate(preds, labels):\n",
    "    def to_label(x):\n",
    "        return \"Negative\" if x < -0.3 else \"Positive\" if x > 0.3 else \"Neutral\"\n",
    "    preds_label = [to_label(p) for p in preds]\n",
    "    labels_label = [to_label(l) for l in labels]\n",
    "\n",
    "    macro_f1 = f1_score(labels_label, preds_label, average=\"macro\")\n",
    "    micro_f1 = f1_score(labels_label, preds_label, average=\"micro\")\n",
    "    recall = recall_score(labels_label, preds_label, average=None, labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    acc = accuracy_score(labels_label, preds_label)\n",
    "    return macro_f1, micro_f1, recall, acc\n",
    "\n",
    "# ---- FEATURE EXTRACTOR ----\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "video_mae = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "video_mae.eval()\n",
    "for param in video_mae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ---- LOAD DATASET AND SPLIT BASED ON CSV ----\n",
    "full_dataset = VideoClipDataset(clip_dir, mapping_csv, feature_extractor)\n",
    "\n",
    "# Load the CSV again to fetch split info per clip\n",
    "df = pd.read_csv(mapping_csv)\n",
    "\n",
    "# Extract clip-level split indices\n",
    "train_indices = df[df['split'] == 'train'].index.tolist()\n",
    "val_indices   = df[df['split'] == 'val'].index.tolist()\n",
    "test_indices  = df[df['split'] == 'test'].index.tolist()\n",
    "\n",
    "# Create datasets using clip-level splits\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset   = Subset(full_dataset, val_indices)\n",
    "test_dataset  = Subset(full_dataset, test_indices)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---- MODEL SETUP ----\n",
    "regressor = SentimentRegressor(feature_dim=768).to(device)\n",
    "loss_fn = CenteredWeightedMSELoss()  # or use CenteredWeightedMSELoss()\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=2e-4)\n",
    "\n",
    "# ---- TRAIN LOOP ----\n",
    "best_macro_f1 = -np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    train_loss, _, _ = run_epoch(regressor, train_loader, optimizer, is_train=True)\n",
    "    val_loss, val_preds, val_labels = run_epoch(regressor, val_loader, optimizer, is_train=False)\n",
    "\n",
    "    macro_f1, micro_f1, recall, acc = evaluate(val_preds, val_labels)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(regressor.state_dict(), \"best_regressor_Customized_Loss_final.pth\")\n",
    "        print(f\"✅ Best model saved at epoch {epoch} with Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "# ---- TEST EVALUATION ----\n",
    "test_loss, test_preds, test_labels = run_epoch(regressor, test_loader, optimizer, is_train=False)\n",
    "macro_f1, micro_f1, recall, acc = evaluate(test_preds, test_labels)\n",
    "print(\"\\n----- TEST RESULTS -----\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
