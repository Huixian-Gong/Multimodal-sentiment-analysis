{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbf5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Train Loss: 0.9663 | Val Loss: 0.9873\n",
      "Macro-F1: 0.2786 | Micro-F1: 0.6168 | Acc: 0.6168 | Recall: [0.         0.04761905 0.91549296]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 2\n",
      "Train Loss: 0.9276 | Val Loss: 0.9256\n",
      "Macro-F1: 0.2659 | Micro-F1: 0.6636 | Acc: 0.6636 | Recall: [0. 0. 1.]\n",
      "\n",
      "Epoch 3\n",
      "Train Loss: 0.9283 | Val Loss: 0.9317\n",
      "Macro-F1: 0.2659 | Micro-F1: 0.6636 | Acc: 0.6636 | Recall: [0. 0. 1.]\n",
      "\n",
      "Epoch 4\n",
      "Train Loss: 0.8941 | Val Loss: 0.9305\n",
      "Macro-F1: 0.2614 | Micro-F1: 0.6449 | Acc: 0.6449 | Recall: [0.         0.         0.97183099]\n",
      "\n",
      "Epoch 5\n",
      "Train Loss: 0.8939 | Val Loss: 0.9582\n",
      "Macro-F1: 0.2971 | Micro-F1: 0.5794 | Acc: 0.5794 | Recall: [0.         0.14285714 0.83098592]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 6\n",
      "Train Loss: 0.9051 | Val Loss: 0.9612\n",
      "Macro-F1: 0.3074 | Micro-F1: 0.5140 | Acc: 0.5140 | Recall: [0.         0.33333333 0.67605634]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 7\n",
      "Train Loss: 0.8737 | Val Loss: 0.9493\n",
      "Macro-F1: 0.3123 | Micro-F1: 0.5607 | Acc: 0.5607 | Recall: [0.         0.23809524 0.77464789]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 8\n",
      "Train Loss: 0.8509 | Val Loss: 1.0022\n",
      "Macro-F1: 0.3187 | Micro-F1: 0.4579 | Acc: 0.4579 | Recall: [0.         0.76190476 0.46478873]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 9\n",
      "Train Loss: 0.8148 | Val Loss: 0.9647\n",
      "Macro-F1: 0.3063 | Micro-F1: 0.5701 | Acc: 0.5701 | Recall: [0.         0.19047619 0.8028169 ]\n",
      "\n",
      "Epoch 10\n",
      "Train Loss: 0.7634 | Val Loss: 0.8679\n",
      "Macro-F1: 0.4574 | Micro-F1: 0.6168 | Acc: 0.6168 | Recall: [0.13333333 0.52380952 0.74647887]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 11\n",
      "Train Loss: 0.8066 | Val Loss: 0.9831\n",
      "Macro-F1: 0.3133 | Micro-F1: 0.6168 | Acc: 0.6168 | Recall: [0.         0.14285714 0.88732394]\n",
      "\n",
      "Epoch 12\n",
      "Train Loss: 0.7853 | Val Loss: 0.8489\n",
      "Macro-F1: 0.4697 | Micro-F1: 0.6355 | Acc: 0.6355 | Recall: [0.2        0.38095238 0.8028169 ]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 13\n",
      "Train Loss: 0.7484 | Val Loss: 0.9308\n",
      "Macro-F1: 0.3804 | Micro-F1: 0.5701 | Acc: 0.5701 | Recall: [0.         0.71428571 0.64788732]\n",
      "\n",
      "Epoch 14\n",
      "Train Loss: 0.7033 | Val Loss: 0.9399\n",
      "Macro-F1: 0.3615 | Micro-F1: 0.6075 | Acc: 0.6075 | Recall: [0.06666667 0.19047619 0.84507042]\n",
      "\n",
      "Epoch 15\n",
      "Train Loss: 0.7200 | Val Loss: 0.8269\n",
      "Macro-F1: 0.4998 | Micro-F1: 0.6729 | Acc: 0.6729 | Recall: [0.2        0.42857143 0.84507042]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 16\n",
      "Train Loss: 0.6787 | Val Loss: 0.9481\n",
      "Macro-F1: 0.4582 | Micro-F1: 0.5234 | Acc: 0.5234 | Recall: [0.2        0.85714286 0.49295775]\n",
      "\n",
      "Epoch 17\n",
      "Train Loss: 0.6367 | Val Loss: 1.0084\n",
      "Macro-F1: 0.3610 | Micro-F1: 0.6355 | Acc: 0.6355 | Recall: [0.06666667 0.14285714 0.90140845]\n",
      "\n",
      "Epoch 18\n",
      "Train Loss: 0.7194 | Val Loss: 0.9571\n",
      "Macro-F1: 0.3029 | Micro-F1: 0.5888 | Acc: 0.5888 | Recall: [0.         0.14285714 0.84507042]\n",
      "\n",
      "Epoch 19\n",
      "Train Loss: 0.6851 | Val Loss: 0.8363\n",
      "Macro-F1: 0.5349 | Micro-F1: 0.7103 | Acc: 0.7103 | Recall: [0.4        0.19047619 0.92957746]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 20\n",
      "Train Loss: 0.6072 | Val Loss: 0.9456\n",
      "Macro-F1: 0.5179 | Micro-F1: 0.6355 | Acc: 0.6355 | Recall: [0.2        0.76190476 0.69014085]\n",
      "\n",
      "Epoch 21\n",
      "Train Loss: 0.6402 | Val Loss: 0.8877\n",
      "Macro-F1: 0.4853 | Micro-F1: 0.5981 | Acc: 0.5981 | Recall: [0.2        0.61904762 0.67605634]\n",
      "\n",
      "Epoch 22\n",
      "Train Loss: 0.6820 | Val Loss: 0.9265\n",
      "Macro-F1: 0.4749 | Micro-F1: 0.6355 | Acc: 0.6355 | Recall: [0.2        0.38095238 0.8028169 ]\n",
      "\n",
      "Epoch 23\n",
      "Train Loss: 0.6742 | Val Loss: 0.8653\n",
      "Macro-F1: 0.4997 | Micro-F1: 0.6075 | Acc: 0.6075 | Recall: [0.2        0.71428571 0.66197183]\n",
      "\n",
      "Epoch 24\n",
      "Train Loss: 0.5850 | Val Loss: 0.8772\n",
      "Macro-F1: 0.5335 | Micro-F1: 0.6168 | Acc: 0.6168 | Recall: [0.53333333 0.38095238 0.70422535]\n",
      "\n",
      "Epoch 25\n",
      "Train Loss: 0.5713 | Val Loss: 0.8902\n",
      "Macro-F1: 0.4853 | Micro-F1: 0.6636 | Acc: 0.6636 | Recall: [0.26666667 0.28571429 0.85915493]\n",
      "\n",
      "Epoch 26\n",
      "Train Loss: 0.5686 | Val Loss: 0.9549\n",
      "Macro-F1: 0.5296 | Micro-F1: 0.6449 | Acc: 0.6449 | Recall: [0.2        0.80952381 0.69014085]\n",
      "\n",
      "Epoch 27\n",
      "Train Loss: 0.5986 | Val Loss: 0.9445\n",
      "Macro-F1: 0.5269 | Micro-F1: 0.5607 | Acc: 0.5607 | Recall: [0.66666667 0.57142857 0.53521127]\n",
      "\n",
      "Epoch 28\n",
      "Train Loss: 0.5856 | Val Loss: 0.8625\n",
      "Macro-F1: 0.4681 | Micro-F1: 0.6355 | Acc: 0.6355 | Recall: [0.13333333 0.52380952 0.77464789]\n",
      "\n",
      "Epoch 29\n",
      "Train Loss: 0.5827 | Val Loss: 1.0241\n",
      "Macro-F1: 0.4971 | Micro-F1: 0.6916 | Acc: 0.6916 | Recall: [0.13333333 0.47619048 0.87323944]\n",
      "\n",
      "Epoch 30\n",
      "Train Loss: 0.6183 | Val Loss: 0.9519\n",
      "Macro-F1: 0.4839 | Micro-F1: 0.6355 | Acc: 0.6355 | Recall: [0.13333333 0.66666667 0.73239437]\n",
      "\n",
      "Epoch 31\n",
      "Train Loss: 0.5509 | Val Loss: 0.9067\n",
      "Macro-F1: 0.4933 | Micro-F1: 0.6542 | Acc: 0.6542 | Recall: [0.2        0.42857143 0.81690141]\n",
      "\n",
      "Epoch 32\n",
      "Train Loss: 0.5417 | Val Loss: 1.0232\n",
      "Macro-F1: 0.4936 | Micro-F1: 0.5701 | Acc: 0.5701 | Recall: [0.26666667 0.66666667 0.6056338 ]\n",
      "\n",
      "Epoch 33\n",
      "Train Loss: 0.5438 | Val Loss: 1.0927\n",
      "Macro-F1: 0.4583 | Micro-F1: 0.6636 | Acc: 0.6636 | Recall: [0.06666667 0.57142857 0.81690141]\n",
      "\n",
      "Epoch 34\n",
      "Train Loss: 0.5700 | Val Loss: 0.8993\n",
      "Macro-F1: 0.5081 | Micro-F1: 0.6916 | Acc: 0.6916 | Recall: [0.2        0.38095238 0.88732394]\n",
      "\n",
      "Epoch 35\n",
      "Train Loss: 0.5277 | Val Loss: 0.9414\n",
      "Macro-F1: 0.5609 | Micro-F1: 0.6168 | Acc: 0.6168 | Recall: [0.53333333 0.61904762 0.63380282]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 36\n",
      "Train Loss: 0.5671 | Val Loss: 1.0421\n",
      "Macro-F1: 0.4795 | Micro-F1: 0.5140 | Acc: 0.5140 | Recall: [0.33333333 0.71428571 0.49295775]\n",
      "\n",
      "Epoch 37\n",
      "Train Loss: 0.5541 | Val Loss: 0.8963\n",
      "Macro-F1: 0.4806 | Micro-F1: 0.6262 | Acc: 0.6262 | Recall: [0.2        0.47619048 0.76056338]\n",
      "\n",
      "Epoch 38\n",
      "Train Loss: 0.5409 | Val Loss: 0.9903\n",
      "Macro-F1: 0.4367 | Micro-F1: 0.6262 | Acc: 0.6262 | Recall: [0.06666667 0.57142857 0.76056338]\n",
      "\n",
      "Epoch 39\n",
      "Train Loss: 0.4779 | Val Loss: 0.9839\n",
      "Macro-F1: 0.5281 | Micro-F1: 0.6916 | Acc: 0.6916 | Recall: [0.2        0.52380952 0.84507042]\n",
      "\n",
      "Epoch 40\n",
      "Train Loss: 0.4898 | Val Loss: 1.0384\n",
      "Macro-F1: 0.5177 | Micro-F1: 0.5794 | Acc: 0.5794 | Recall: [0.46666667 0.52380952 0.61971831]\n",
      "\n",
      "Epoch 41\n",
      "Train Loss: 0.4699 | Val Loss: 0.9218\n",
      "Macro-F1: 0.5944 | Micro-F1: 0.6916 | Acc: 0.6916 | Recall: [0.53333333 0.42857143 0.8028169 ]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 42\n",
      "Train Loss: 0.4586 | Val Loss: 0.9572\n",
      "Macro-F1: 0.6240 | Micro-F1: 0.7009 | Acc: 0.7009 | Recall: [0.53333333 0.47619048 0.8028169 ]\n",
      "✅New best model saved.\n",
      "\n",
      "Epoch 43\n",
      "Train Loss: 0.4368 | Val Loss: 1.1218\n",
      "Macro-F1: 0.5525 | Micro-F1: 0.5888 | Acc: 0.5888 | Recall: [0.6        0.66666667 0.56338028]\n",
      "\n",
      "Epoch 44\n",
      "Train Loss: 0.5485 | Val Loss: 0.9897\n",
      "Macro-F1: 0.5280 | Micro-F1: 0.6168 | Acc: 0.6168 | Recall: [0.4        0.42857143 0.71830986]\n",
      "\n",
      "Epoch 45\n",
      "Train Loss: 0.4758 | Val Loss: 1.1102\n",
      "Macro-F1: 0.4654 | Micro-F1: 0.6075 | Acc: 0.6075 | Recall: [0.13333333 0.66666667 0.69014085]\n",
      "\n",
      "Epoch 46\n",
      "Train Loss: 0.4443 | Val Loss: 1.0308\n",
      "Macro-F1: 0.5596 | Micro-F1: 0.6449 | Acc: 0.6449 | Recall: [0.33333333 0.66666667 0.70422535]\n",
      "\n",
      "Epoch 47\n",
      "Train Loss: 0.4486 | Val Loss: 1.0154\n",
      "Macro-F1: 0.5220 | Micro-F1: 0.6729 | Acc: 0.6729 | Recall: [0.53333333 0.19047619 0.84507042]\n",
      "\n",
      "Epoch 48\n",
      "Train Loss: 0.3913 | Val Loss: 1.1000\n",
      "Macro-F1: 0.5142 | Micro-F1: 0.6729 | Acc: 0.6729 | Recall: [0.2        0.52380952 0.81690141]\n",
      "\n",
      "Epoch 49\n",
      "Train Loss: 0.3560 | Val Loss: 1.1414\n",
      "Macro-F1: 0.5147 | Micro-F1: 0.6636 | Acc: 0.6636 | Recall: [0.26666667 0.42857143 0.81690141]\n",
      "\n",
      "Epoch 50\n",
      "Train Loss: 0.3776 | Val Loss: 1.2000\n",
      "Macro-F1: 0.5558 | Micro-F1: 0.5981 | Acc: 0.5981 | Recall: [0.66666667 0.57142857 0.5915493 ]\n",
      "\n",
      "FINAL TEST EVALUATION\n",
      "Test Loss : 1.0603\n",
      "Macro-F1  : 0.5298\n",
      "Micro-F1  : 0.5614\n",
      "Accuracy  : 0.5614\n",
      "Recall    : [0.625      0.44827586 0.5942029 ]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gpu_ids = [4]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, gpu_ids))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# ---- SET GLOBAL SEED ----\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ---- SETTINGS ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# clip_dir = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI/Clip/Clips_16frames\"\n",
    "mapping_csv = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI-Seg/Labels/new_sentiment_split_2.csv\"\n",
    "clip_dir = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI-Seg/Clip/Clips_16frames\"\n",
    "# mapping_csv = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI-Seg/Clip/clip_balanced_split.csv\"\n",
    "batch_size = 64\n",
    "clip_len = 16\n",
    "num_epochs = 20\n",
    "\n",
    "# ---- DATASET ----\n",
    "class VideoClipDataset(Dataset):\n",
    "    def __init__(self, clip_dir, csv_path, feature_extractor, transform=False, duplicate_flipped=False):\n",
    "        self.clip_dir = clip_dir\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transform = transform\n",
    "        self.duplicate_flipped = duplicate_flipped\n",
    "\n",
    "        self.samples = list(self.df.itertuples(index=False))\n",
    "\n",
    "        if self.duplicate_flipped:\n",
    "            self.samples = [(s, False) for s in self.samples] + [(s, True) for s in self.samples]\n",
    "        else:\n",
    "            self.samples = [(s, False) for s in self.samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row, apply_flip = self.samples[idx]\n",
    "        clip_path = os.path.join(self.clip_dir, row.clip_filename_y)\n",
    "\n",
    "        cap = cv2.VideoCapture(clip_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = frame[:, :, ::-1]  # BGR to RGB\n",
    "            if self.transform and apply_flip:\n",
    "                frame = cv2.flip(frame, 1)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < clip_len:\n",
    "            frames += [frames[-1]] * (clip_len - len(frames))\n",
    "        frames = frames[:clip_len]\n",
    "\n",
    "        inputs = self.feature_extractor(images=frames, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        score = row.sentiment_score\n",
    "        if score < -0.3:\n",
    "            label = 0\n",
    "        elif score > 0.3:\n",
    "            label = 2\n",
    "        else:\n",
    "            label = 1\n",
    "\n",
    "        return inputs, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "# ---- MODEL ----\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ---- TRAINING UTILS ----\n",
    "def run_epoch(model, loader, optimizer, is_train=True):\n",
    "    model.train() if is_train else model.eval()\n",
    "    total_preds, total_labels = [], []\n",
    "    total_loss = 0\n",
    "\n",
    "    for clips, targets in tqdm(loader, leave=False):\n",
    "        clips, targets = clips.to(device), targets.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            features = video_mae(clips).last_hidden_state.mean(dim=1)\n",
    "            preds = model(features)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_preds.extend(torch.argmax(preds, dim=1).detach().cpu().numpy())\n",
    "        total_labels.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    return total_loss / len(loader), np.array(total_preds), np.array(total_labels)\n",
    "\n",
    "def evaluate(preds, labels):\n",
    "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    preds_label = [label_map[p] for p in preds]\n",
    "    labels_label = [label_map[l] for l in labels]\n",
    "\n",
    "    macro_f1 = f1_score(labels_label, preds_label, average=\"macro\")\n",
    "    micro_f1 = f1_score(labels_label, preds_label, average=\"micro\")\n",
    "    recall = recall_score(labels_label, preds_label, average=None, labels=[\"Negative\", \"Neutral\", \"Positive\"])\n",
    "    acc = accuracy_score(labels_label, preds_label)\n",
    "    return macro_f1, micro_f1, recall, acc\n",
    "\n",
    "# ---- FEATURE EXTRACTOR ----\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "video_mae = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "video_mae.eval()\n",
    "video_mae.encoder.layer[-1].requires_grad_(True)\n",
    "for param in video_mae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ---- LOAD DATASET AND SPLIT WITH CUSTOM PROPORTIONS ----\n",
    "df = pd.read_csv(mapping_csv)\n",
    "\n",
    "# Define your desired proportions for the training set\n",
    "# Example: use 50% of each class in the training split\n",
    "proportion = {\n",
    "    \"Negative\": 1,\n",
    "    \"Neutral\": 0.8,\n",
    "    \"Positive\": 1,\n",
    "}\n",
    "\n",
    "# Filter only training data\n",
    "train_df = df[df[\"split\"] == \"train\"]\n",
    "\n",
    "# Apply proportional sampling for each class\n",
    "train_samples = []\n",
    "for label, frac in proportion.items():\n",
    "    label_df = train_df[train_df[\"sentiment_label\"] == label]\n",
    "    sampled_df = label_df.sample(frac=frac, random_state=42)\n",
    "    train_samples.append(sampled_df)\n",
    "\n",
    "train_df_balanced = pd.concat(train_samples)\n",
    "train_indices = train_df_balanced.index.tolist()\n",
    "print(f\"📦 Original training dataset size (before flipping): {len(train_indices)} samples\")\n",
    "\n",
    "# Validation and test sets remain unchanged\n",
    "val_indices = df[df[\"split\"] == \"val\"].index.tolist()\n",
    "test_indices = df[df[\"split\"] == \"test\"].index.tolist()\n",
    "\n",
    "# Reload dataset (to avoid using the whole dataset)\n",
    "full_dataset = VideoClipDataset(clip_dir, mapping_csv, feature_extractor)\n",
    "# Reload dataset with optional transform for training\n",
    "full_dataset_train = VideoClipDataset(\n",
    "    clip_dir, mapping_csv, feature_extractor,\n",
    "    transform=True, duplicate_flipped=True\n",
    ")\n",
    "full_dataset_val   = VideoClipDataset(clip_dir, mapping_csv, feature_extractor, transform=False)\n",
    "full_dataset_test  = VideoClipDataset(clip_dir, mapping_csv, feature_extractor, transform=False)\n",
    "\n",
    "# Extend train_indices to also include the flipped versions (shifted by len of original dataset)\n",
    "doubled_train_indices = train_indices + [i + len(full_dataset.samples) // 2 for i in train_indices]\n",
    "train_dataset = Subset(full_dataset_train, doubled_train_indices)\n",
    "\n",
    "\n",
    "# train_dataset = Subset(full_dataset_train, train_indices)\n",
    "val_dataset   = Subset(full_dataset_val, val_indices)\n",
    "test_dataset  = Subset(full_dataset_test, test_indices)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "print(f\"🔍 Training dataset size (with flipping): {len(train_loader.dataset)} samples\")\n",
    "\n",
    "\n",
    "# ---- TRAINING SETUP ----\n",
    "model = SentimentClassifier(feature_dim=768).to(device)\n",
    "class_weights = torch.tensor([1.0, 0.8, 1.3], dtype=torch.float).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "\n",
    "# ---- TRAIN LOOP ----\n",
    "best_macro_f1 = -np.inf\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    train_loss, _, _ = run_epoch(model, train_loader, optimizer, is_train=True)\n",
    "    val_loss, val_preds, val_labels = run_epoch(model, val_loader, optimizer, is_train=False)\n",
    "\n",
    "    macro_f1, micro_f1, recall, acc = evaluate(val_preds, val_labels)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")\n",
    "    # scheduler.step(macro_f1)\n",
    "\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        torch.save(model.state_dict(), \"best_classifier_CE_final.pth\")\n",
    "        print(f\"✅ Best model saved at epoch {epoch} with Macro-F1={macro_f1:.4f}\")\n",
    "\n",
    "# ---- TEST EVALUATION ----\n",
    "test_loss, test_preds, test_labels = run_epoch(model, test_loader, optimizer, is_train=False)\n",
    "macro_f1, micro_f1, recall, acc = evaluate(test_preds, test_labels)\n",
    "print(\"\\n----- TEST RESULTS -----\")\n",
    "print(f\"Macro-F1: {macro_f1:.4f} | Micro-F1: {micro_f1:.4f} | Acc: {acc:.4f} | Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---- SAVE TEST RESULTS ----\n",
    "# label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "# pred_labels = [label_map[p] for p in test_preds]\n",
    "\n",
    "# # Get filenames from the original DataFrame using test indices\n",
    "# df_test = df.iloc[test_indices].reset_index(drop=True)\n",
    "# filenames = df_test[\"clip_filename\"].tolist()\n",
    "\n",
    "# # Create DataFrame and save to CSV\n",
    "# results_df = pd.DataFrame({\n",
    "#     \"filename\": filenames,\n",
    "#     \"sentiment_class\": pred_labels\n",
    "# })\n",
    "# results_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "# print(\"📁 Test predictions saved to 'test_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72943d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# from collections import Counter\n",
    "\n",
    "# # Load CSV\n",
    "# df = pd.read_csv(\"test_predictions.csv\")\n",
    "\n",
    "# # Optional: If sentiment_class is still in string form, convert to numeric\n",
    "# label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "# if df[\"sentiment_class\"].dtype == object:\n",
    "#     df[\"sentiment_class\"] = df[\"sentiment_class\"].map(label_map)\n",
    "\n",
    "# # Strip clip suffix: anything like \"_<number>_clip<number>.mp4\"\n",
    "# def extract_base_filename(fname):\n",
    "#     return re.sub(r\"_\\d+_clip\\d+\\.mp4$\", \"\", fname)\n",
    "\n",
    "# df[\"base_filename\"] = df[\"filename\"].apply(extract_base_filename)\n",
    "\n",
    "# # Majority vote\n",
    "# def majority_vote(group):\n",
    "#     vote = Counter(group[\"sentiment_class\"]).most_common(1)[0][0]\n",
    "#     return pd.Series({\"sentiment_class\": vote})\n",
    "\n",
    "# aggregated_df = df.groupby(\"base_filename\").apply(majority_vote).reset_index()\n",
    "# aggregated_df.columns = [\"filename\", \"sentiment_class\"]\n",
    "\n",
    "# # Save result\n",
    "# aggregated_df.to_csv(\"test_predictions_aggregated.csv\", index=False)\n",
    "# print(\"✅ Aggregated predictions saved to 'test_predictions_aggregated.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01b1ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/huixian/.conda/envs/multiood/lib/python3.12/site-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_4001160/2911371812.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
      "100%|██████████| 9/9 [00:29<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved clip-level predictions to: test_predictions_with_video_id.csv\n",
      "✅ Saved video-level majority vote to: video_level_majority_predictions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "gpu_ids = [4]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, gpu_ids))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEModel\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- SET GLOBAL SEED ----\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ---- SETTINGS ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mapping_csv = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI-Seg/Labels/new_sentiment_split_2.csv\"\n",
    "clip_dir = \"/data/home/huixian/Documents/Homeworks/535_project/MOSEI-Seg/Clip/Clips_16frames\"\n",
    "batch_size = 64\n",
    "clip_len = 16\n",
    "checkpoint_path = \"/data/home/huixian/Documents/Homeworks/535_project/mosei_code/best_classifier_CE_final_0.5796.pth\"\n",
    "\n",
    "# ---- DATASET ----\n",
    "class VideoClipDataset(Dataset):\n",
    "    def __init__(self, clip_dir, csv_path, feature_extractor):\n",
    "        self.clip_dir = clip_dir\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.samples = self.df[self.df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.samples.iloc[idx]\n",
    "        clip_path = os.path.join(self.clip_dir, row[\"clip_filename_y\"])\n",
    "\n",
    "        cap = cv2.VideoCapture(clip_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = frame[:, :, ::-1]  # BGR to RGB\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) < clip_len:\n",
    "            frames += [frames[-1]] * (clip_len - len(frames))\n",
    "        frames = frames[:clip_len]\n",
    "\n",
    "        inputs = self.feature_extractor(images=frames, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        return inputs, row[\"video_id\"], row[\"clip_filename_y\"]\n",
    "\n",
    "# ---- MODEL ----\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# ---- EVALUATION UTILS ----\n",
    "def run_test(model, loader):\n",
    "    model.eval()\n",
    "    preds, video_ids, filenames = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for clips, vids, fnames in tqdm(loader):\n",
    "            clips = clips.to(device)\n",
    "            features = video_mae(clips).last_hidden_state.mean(dim=1)\n",
    "            logits = model(features)\n",
    "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(predictions)\n",
    "            video_ids.extend(vids)\n",
    "            filenames.extend(fnames)\n",
    "\n",
    "    return preds, video_ids, filenames\n",
    "\n",
    "# ---- LOAD FEATURE EXTRACTOR AND MODEL ----\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\n",
    "video_mae = VideoMAEModel.from_pretrained(\"MCG-NJU/videomae-base\").to(device)\n",
    "video_mae.eval()\n",
    "\n",
    "model = SentimentClassifier(feature_dim=768).to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ---- LOAD DATASET ----\n",
    "test_dataset = VideoClipDataset(clip_dir, mapping_csv, feature_extractor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ---- RUN INFERENCE ----\n",
    "preds, video_ids, filenames = run_test(model, test_loader)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# ---- CONVERT TO LABELS ----\n",
    "int_to_label = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "label_preds = [int_to_label[p] for p in preds]\n",
    "\n",
    "# ---- SAVE CLIP-LEVEL PREDICTIONS ----\n",
    "clip_df = pd.DataFrame({\n",
    "    \"clip_filename\": filenames,\n",
    "    \"video_id\": video_ids,\n",
    "    \"predicted_label\": label_preds\n",
    "})\n",
    "clip_df.to_csv(\"test_predictions_with_video_id.csv\", index=False)\n",
    "print(\"✅ Saved clip-level predictions to: test_predictions_with_video_id.csv\")\n",
    "\n",
    "# ---- MAJORITY VOTING PER VIDEO_ID ----\n",
    "majority_vote = (\n",
    "    clip_df.groupby(\"video_id\")[\"predicted_label\"]\n",
    "    .apply(lambda x: Counter(x).most_common(1)[0][0])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"predicted_label\": \"majority_label\"})\n",
    ")\n",
    "\n",
    "majority_vote.to_csv(\"video_level_majority_predictions.csv\", index=False)\n",
    "print(\"✅ Saved video-level majority vote to: video_level_majority_predictions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ff06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 Number of unique video_id entries: 114\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_path = \"/data/home/huixian/Documents/Homeworks/535_project/late_fusion/audio_RNN_preds.csv\"\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Count unique video_id values\n",
    "unique_video_ids = df[\"video_id\"].nunique()\n",
    "\n",
    "print(f\"🎥 Number of unique video_id entries: {unique_video_ids}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
